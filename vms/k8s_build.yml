---
- hosts: pvenodes[0]
  become: false
  gather_facts: false
  tasks:
    - name: Add master nodes to inventory
      add_host:
        name: "{{ item.name }}"
        ansible_host: "{{ item.ipv4_address.split('/')[0] }}"
        ansible_become_password: "{{ sudo_pass }}"
        ansible_ssh_private_key_file: "{{ ansible_key_file | replace('.pub', '') }}"
        ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
        groups:
          - masters
          - k8s_nodes
      loop: '{{ computers }}'
      when: item.role == "master"

    - name: Add worker nodes to inventory
      add_host:
        name: "{{ item.name }}"
        ansible_host: "{{ item.ipv4_address.split('/')[0] }}"
        ansible_become_password: "{{ sudo_pass }}"
        ansible_ssh_private_key_file: "{{ ansible_key_file | replace('.pub', '') }}"
        ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
        groups:
          - workers
          - k8s_nodes
      loop: '{{ computers }}'
      when: item.role == "worker"

- name: Set up Keepalived for Kubernetes VIP
  hosts: masters
  become: true
  gather_facts: false
  vars:
    vip_address: "10.55.10.200"
    vip_interface: "eth0"  # <-- adjust if needed
    vip_password: "42K8Skeep"
    vip_router_id: 51
    master_priority_offset: 100
  tasks:
    - name: Install keepalived
      apt:
        name: keepalived
        state: present
        update_cache: true

    - name: Determine master priority
      set_fact:
        vip_priority: "{{ master_priority_offset - groups['masters'].index(inventory_hostname) }}"

    - name: Configure keepalived
      copy:
        dest: /etc/keepalived/keepalived.conf
        content: |
          vrrp_instance VI_1 {
              state BACKUP
              interface {{ vip_interface }}
              virtual_router_id {{ vip_router_id }}
              priority {{ vip_priority }}
              advert_int 1
              authentication {
                  auth_type PASS
                  auth_pass {{ vip_password }}
              }
              virtual_ipaddress {
                  {{ vip_address }}
              }
          }
        mode: '0644'

    - name: Enable and restart keepalived
      systemd:
        name: keepalived
        enabled: true
        state: restarted

    - name: Check VIP status
      shell: ip a | grep {{ vip_address }}
      register: vip_check
      changed_when: false
      failed_when: false

    - name: Show VIP ownership
      debug:
        msg: "VIP {{ vip_address }} is {{ 'present' if vip_check.stdout else 'absent' }} on this node."

- hosts: k8s_nodes
  become: true
  gather_facts: false
  tasks:
    - name: Install Kubernetes prerequisites
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
        state: present
        update_cache: true

    - name: Add Kubernetes APT keyring
      shell: |
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key \
        | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: Add Kubernetes APT repository
      copy:
        dest: /etc/apt/sources.list.d/kubernetes.list
        content: |
          deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /

    - name: Install kubelet, kubeadm, and kubectl
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: true

    - name: Hold kubelet, kubeadm, and kubectl at current version
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: Install containerd
      apt:
        name: containerd
        state: present
        update_cache: true

    - name: Create containerd config directory
      file:
        path: /etc/containerd
        state: directory
        mode: '0755'

    - name: Generate default containerd config
      shell: containerd config default > /etc/containerd/config.toml
      args:
        creates: /etc/containerd/config.toml

    - name: Enable and restart containerd
      systemd:
        name: containerd
        enabled: true
        state: restarted

    - name: Enable IP forwarding temporarily
      sysctl:
        name: net.ipv4.ip_forward
        value: '1'
        state: present
        reload: yes

    - name: Ensure IP forwarding is persistent
      copy:
        dest: /etc/sysctl.d/99-kubernetes-ipforward.conf
        content: |
          net.ipv4.ip_forward=1
        mode: '0644'

    - name: Reload sysctl settings
      command: sysctl --system

- hosts: "{{ groups['masters'][0] }}"
  become: true
  gather_facts: false
  vars:
    reset_k8s_node: false
  tasks:
    - name: Reset kubeadm if needed
      command: kubeadm reset -f
      when: reset_k8s_node

    - name: Clean Kubernetes directories
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes
        - /var/lib/etcd
        - /var/lib/kubelet
        - /etc/cni
        - /opt/cni
      when: reset_k8s_node

    - name: Flush iptables rules
      command: iptables -F
      when: reset_k8s_node

    - name: Delete default CNI interfaces
      shell: |
        ip link delete cni0 || true
        ip link delete flannel.1 || true
      ignore_errors: true
      when: reset_k8s_node

    - name: Initialize Kubernetes first master
      vars:
        vip_address: 10.55.10.200
      command: >
        kubeadm init
        --control-plane-endpoint "{{ vip_address }}:6443"
        --pod-network-cidr="{{ hostvars[groups['pvenodes'][0]].computers[0].ipv4_address | ipaddr('network/prefix') }}"
        --upload-certs
        --skip-phases=addon/coredns
      register: kubeadm_output

    - name: Extract bootstrap token
      set_fact:
        kubeadm_token: "{{ (kubeadm_output.stdout | regex_findall('Using token: ([a-z0-9\\.]+)')).0 }}"

    - name: Extract certificate key
      set_fact:
        kubeadm_cert_key: "{{ (kubeadm_output.stdout | regex_findall('certificate key:\\s*([a-f0-9]{64})')).0 }}"

    - name: Extract CA cert hash
      set_fact:
        kubeadm_ca_cert_hash: "{{ (kubeadm_output.stdout | regex_findall('sha256:[a-f0-9]{64}')).0 }}"

    - name: Construct control-plane join command
      set_fact:
        kubeadm_cp_join: >-
          kubeadm join {{ hostvars[groups['pvenodes'][0]].computers[0].ipv4_address.split('/')[0] }}:6443
          --token {{ kubeadm_token }}
          --discovery-token-ca-cert-hash {{ kubeadm_ca_cert_hash }}
          --control-plane
          --certificate-key {{ kubeadm_cert_key }}

    - name: Construct worker join command
      set_fact:
        kubeadm_worker_join: >-
          kubeadm join {{ hostvars[groups['pvenodes'][0]].computers[0].ipv4_address.split('/')[0] }}:6443
          --token {{ kubeadm_token }}
          --discovery-token-ca-cert-hash {{ kubeadm_ca_cert_hash }}

    - name: Create .kube config directory
      file:
        path: "/home/{{ default_user }}/.kube"
        state: directory
        mode: '0755'
        owner: "{{ default_user }}"
        group: "{{ default_user }}"

    - name: Copy admin.conf for kubectl
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "/home/{{ default_user }}/.kube/config"
        remote_src: true
        owner: "{{ default_user }}"
        group: "{{ default_user }}"
        mode: '0600'

    - name: Download Cilium CLI
      get_url:
        url: https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz
        dest: /tmp/cilium-linux-amd64.tar.gz
        mode: '0644'

    - name: Download Cilium CLI SHA256 checksum
      get_url:
        url: https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz.sha256sum
        dest: /tmp/cilium-linux-amd64.tar.gz.sha256sum
        mode: '0644'

    - name: Verify checksum
      shell: sha256sum --check /tmp/cilium-linux-amd64.tar.gz.sha256sum
      args:
        chdir: /tmp
      register: cilium_checksum
      failed_when: "'OK' not in cilium_checksum.stdout"

    - name: Extract and install Cilium CLI
      unarchive:
        src: /tmp/cilium-linux-amd64.tar.gz
        dest: /usr/local/bin/
        remote_src: yes
        creates: /usr/local/bin/cilium

    - name: Debug extracted values
      debug:
        msg: |
          Token: {{ kubeadm_token }}
          Cert Key: {{ kubeadm_cert_key }}
          CA Hash: {{ kubeadm_ca_cert_hash }}

- hosts: "{{ groups['masters'][1:] }}"
  become: true
  gather_facts: false
  vars:
    reset_k8s_node: false
  tasks:
    - name: Reset Kubernetes on secondary control plane
      command: kubeadm reset -f

    - name: Remove Kubernetes config directories (secondary)
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes
        - /var/lib/etcd
        - /var/lib/kubelet
        - /etc/cni
        - /opt/cni
      when: reset_k8s_node

    - name: Flush iptables rules
      command: iptables -F
      when: reset_k8s_node

    - name: Delete default CNI interfaces
      shell: |
        ip link delete cni0 || true
        ip link delete flannel.1 || true
      ignore_errors: true
      when: reset_k8s_node

    - name: Join secondary masters to control plane
      shell: "{{ hostvars[groups['masters'][0]].kubeadm_cp_join }}"
      when: hostvars[groups['masters'][0]].kubeadm_cp_join is defined

- hosts: workers
  become: true
  gather_facts: false
  vars:
    reset_k8s_node: false
  tasks:
    - name: Reset worker node
      command: kubeadm reset -f
      when: reset_k8s_node

    - name: Remove Kubernetes config directories
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes
        - /var/lib/kubelet
        - /etc/cni
        - /opt/cni
      when: reset_k8s_node

    - name: Flush iptables rules
      command: iptables -F
      when: reset_k8s_node

    - name: Delete CNI interfaces
      shell: |
        ip link delete cni0 || true
        ip link delete flannel.1 || true
      ignore_errors: true
      when: reset_k8s_node

    - name: Join worker nodes to the cluster
      shell: "{{ hostvars[groups['masters'][0]].kubeadm_worker_join }}"
      when: hostvars[groups['masters'][0]].kubeadm_worker_join is defined